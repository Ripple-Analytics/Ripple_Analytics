# Mental Models System - Autonomous Iteration Plan #3
**Date:** January 18, 2026
**Trigger:** Recurring autonomous iteration (6-hour cycle)
**Status:** In Progress

## Current State Analysis

### Test Results
- **Passing:** 315 tests
- **Failing:** 6 tests
- **Success Rate:** 98.1% (315/321)

### Failing Tests
1. `test_signal_to_lollapalooza_workflow` - Only detecting 2 signals instead of 3+
2. `test_regime_detection_workflow` - ValueError: 1D array passed instead of 2D
3. `test_knowledge_graph_workflow` - No related models found
4. `test_failure_mode_detection_workflow` - No failure modes detected
5. `test_api_integration_workflow` - 404 error on API endpoint
6. `test_large_text_analysis_performance` - No signals detected in large text

### Improvement Engine Recommendations
Top 10 opportunities identified:
1. **Expand Failure Modes Database** (Impact: 0.90, Effort: Large)
2. **Add Real-World Examples Database** (Impact: 0.80, Effort: Large)
3. **Add Integration Test Suite** (Impact: 0.85, Effort: Medium) - ALREADY COMPLETED
4. **Generate API Documentation** (Impact: 0.80, Effort: Medium)
5. **Improve Error Handling** (Impact: 0.75, Effort: Medium) - ALREADY COMPLETED
6. **Add Decision Journal Feature** (Impact: 0.90, Effort: Large) - ALREADY COMPLETED

## High-Magnitude Iteration Goals

### Primary Objective
Fix all 6 failing tests and implement at least 2 high-impact enhancements.

### Secondary Objectives
1. Improve signal detection accuracy
2. Fix regime detection array shape issues
3. Enhance knowledge graph connectivity
4. Expand failure mode detection
5. Add new high-value features

## Implementation Plan

### Phase 1: Fix Failing Tests (High Priority)

#### 1.1 Fix Signal Detection Issues
**Problem:** Only 2 signals detected instead of 3+
**Root Cause:** Signal detection thresholds may be too strict or text analysis not comprehensive
**Solution:**
- Review signal detection algorithm in `src/analysis/signal_detector.py`
- Adjust relevance score thresholds
- Enhance text analysis to capture more nuanced signals
- Add more mental model patterns

#### 1.2 Fix Regime Detection Array Shape
**Problem:** ValueError - 1D array passed instead of 2D
**Root Cause:** Embedding vector not properly reshaped for sklearn
**Solution:**
- Fix array reshaping in `src/analysis/regime_detector.py`
- Add input validation to ensure proper shape
- Add tests for edge cases

#### 1.3 Fix Knowledge Graph Traversal
**Problem:** No related models found
**Root Cause:** Knowledge graph may not be properly connected or query logic broken
**Solution:**
- Review knowledge graph structure in `src/knowledge_graph/`
- Verify model relationships are properly loaded
- Fix traversal algorithm if needed
- Add more model relationships

#### 1.4 Fix Failure Mode Detection
**Problem:** No failure modes detected
**Root Cause:** Failure mode detection logic may not be triggering
**Solution:**
- Review `src/safeguards/safeguard_engine.py`
- Ensure failure modes are properly loaded
- Adjust detection thresholds
- Add more comprehensive failure mode patterns

#### 1.5 Fix API Integration Test
**Problem:** 404 error on API endpoint
**Root Cause:** API endpoint may not exist or test is calling wrong URL
**Solution:**
- Review test to identify expected endpoint
- Check if API server needs to be running
- Update test or implement missing endpoint

#### 1.6 Fix Large Text Performance Test
**Problem:** No signals detected in large text
**Root Cause:** Similar to 1.1 but for larger text volumes
**Solution:**
- Same fixes as 1.1
- Add chunking for large text if needed
- Optimize performance for large inputs

### Phase 2: High-Impact Enhancements

#### 2.1 Expand Failure Modes Database (Impact: 0.90)
**Goal:** Add failure modes for 20+ additional mental models
**Approach:**
- Identify models without failure modes
- Research common failure patterns
- Add to `src/safeguards/failure_modes_deep.py`
- Write tests for new failure modes

**Target Models (Priority):**
1. Confirmation Bias
2. Availability Heuristic
3. Anchoring
4. Sunk Cost Fallacy
5. Dunning-Kruger Effect
6. Survivorship Bias
7. Hindsight Bias
8. Planning Fallacy
9. Overconfidence Bias
10. Recency Bias
11. Authority Bias
12. Halo Effect
13. Fundamental Attribution Error
14. Self-Serving Bias
15. Groupthink
16. Bandwagon Effect
17. Framing Effect
18. Loss Aversion
19. Status Quo Bias
20. Endowment Effect

#### 2.2 Add Real-World Examples System (Impact: 0.80)
**Goal:** Create structured examples database for mental models
**Approach:**
- Create `src/examples/examples_database.py`
- Define example data structure
- Add 3-5 examples for top 20 mental models
- Integrate with model analyzer
- Add search/retrieval functionality

**Example Structure:**
```python
@dataclass
class RealWorldExample:
    model_id: int
    title: str
    domain: str  # business, personal, historical, scientific
    description: str
    outcome: str
    lessons_learned: List[str]
    source: Optional[str]
    date: Optional[datetime]
```

#### 2.3 Enhanced Signal Detection (Impact: 0.85)
**Goal:** Improve signal detection accuracy by 30%
**Approach:**
- Add more sophisticated NLP patterns
- Implement multi-pass analysis
- Add context-aware scoring
- Use ensemble methods for confidence
- Add signal clustering to identify themes

#### 2.4 Performance Monitoring System (Impact: 0.75)
**Goal:** Add comprehensive performance tracking
**Approach:**
- Create `src/monitoring/performance_monitor.py`
- Track analysis times, accuracy, confidence
- Generate performance reports
- Identify bottlenecks
- Add alerting for degradation

## Success Criteria

### Must Have (Phase 1)
- [ ] All 6 failing tests pass
- [ ] Test success rate: 100% (321/321)
- [ ] No regressions in existing tests
- [ ] Code committed and pushed to GitHub

### Should Have (Phase 2)
- [ ] 20+ new failure modes added
- [ ] Examples database with 50+ examples
- [ ] Signal detection accuracy improved
- [ ] Performance monitoring implemented

### Nice to Have
- [ ] API documentation generated
- [ ] Architecture diagrams updated
- [ ] Performance benchmarks established

## Risk Assessment

### Technical Risks
1. **Test fixes may break other tests** - Mitigation: Run full test suite after each fix
2. **Performance degradation** - Mitigation: Add benchmarks and monitor
3. **Integration issues** - Mitigation: Test incrementally

### Schedule Risks
1. **Scope too large for 6-hour window** - Mitigation: Prioritize test fixes first
2. **Unexpected complexity** - Mitigation: Focus on high-impact items

## Metrics to Track

### Code Quality
- Lines of code added
- Test coverage percentage
- Cyclomatic complexity
- Type hint coverage

### System Performance
- Test execution time
- Analysis latency
- Memory usage
- API response times

### Feature Completeness
- Number of models with failure modes
- Number of real-world examples
- Signal detection accuracy
- Knowledge graph connectivity

## Timeline

**Total Time:** 6 hours
- **Hour 1:** Fix failing tests (1.1-1.3)
- **Hour 2:** Fix failing tests (1.4-1.6)
- **Hour 3:** Expand failure modes database (2.1)
- **Hour 4:** Add examples system (2.2)
- **Hour 5:** Enhanced signal detection (2.3)
- **Hour 6:** Testing, documentation, commit & push

## Charlie Munger Principles Applied

1. **Inversion:** What could make the system fail? â†’ Fix those failure modes
2. **Circle of Competence:** Focus on areas we understand deeply
3. **Margin of Safety:** Add comprehensive error handling and validation
4. **Continuous Learning:** Track outcomes and learn from failures
5. **Mental Models:** Use multiple models to analyze system health

---

*"The best thing a human being can do is to help another human being know more."* - Charlie Munger
